{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d5aa27-12e1-43e7-9eca-84a811b826ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "# from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
    "from torchattacks import PGD\n",
    "from art.utils import random_sphere\n",
    "from art.config import ART_NUMPY_DTYPE\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, vgg19 , VGG19_Weights, resnet18, ResNet18_Weights, densenet121,  densenet121, DenseNet121_Weights, vit_b_16, ViT_B_16_Weights, efficientnet_b2, EfficientNet_B2_Weights,  swin_s, Swin_S_Weights, convnext_base, ConvNeXt_Base_Weights, squeezenet1_0, SqueezeNet1_0_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff6c684-39c6-487b-9f49-6cac58bf103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'569949'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SLURM_JOBID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c49aec-a35a-48a8-a12f-6c808129853e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4049f8-efa8-428e-9ad1-edd186f741e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [(\"vit_b_16\",vit_b_16, ViT_B_16_Weights),\n",
    "          (\"efficientnet_b0\" , efficientnet_b0, EfficientNet_B0_Weights),\n",
    "          (\"swin_s\", swin_s, Swin_S_Weights),\n",
    "          (\"densenet121\" , densenet121, DenseNet121_Weights),\n",
    "          (\"resnet18\" ,resnet18, ResNet18_Weights)]\n",
    "            # (\"convnext_base\", convnext_base,ConvNeXt_Base_Weights)]\n",
    "         # (\"squeezenet1_0\",squeezenet1_0, SqueezeNet1_0_Weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbacab9c-19b5-4c8b-919a-e812f394826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_with_normalization(nn.Module):\n",
    "    def __init__(self, model, normalization):\n",
    "        super(model_with_normalization, self).__init__()\n",
    "        self.model = model \n",
    "        self.normalization = normalization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)\n",
    "        if x.shape == (3,224,224):\n",
    "          x = x.unsqueeze(0)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f85c398-5f55-4665-9605-1c8188208474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Initialize your custom dataset\n",
    "# new_dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d072519-9255-409b-86bd-de643cf905d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1080b-b14c-4713-a3ef-86bc9e2d558b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from torch.cuda.amp import autocast\n",
    "trans = [\n",
    "    transforms.RandomAffine(degrees=(-2, 2), translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(size=(224, 224), padding=10),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=1),\n",
    "    transforms.RandomAutocontrast(1)]\n",
    "\n",
    "\n",
    "augmentations = transforms.Compose(trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9575126-d614-454b-8b07-60d6f1a7cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=12.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df48002a-7a75-4e53-8c8b-e44969e9ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of sample attack.\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import ToTensor, ToPILImage, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# from Normalize import Normalize\n",
    "# from loader import ImageNet\n",
    "# from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "# import pretrainedmodels\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--input_csv', type=str, default='./dataset/images.csv', help='Input directory with images.')\n",
    "# parser.add_argument('--input_dir', type=str, default='./dataset/images', help='Input directory with images.')\n",
    "# parser.add_argument('--output_dir', type=str, default='./outputs/', help='Output directory with adversarial images.')\n",
    "# parser.add_argument('--mean', type=float, default=np.array([0.5, 0.5, 0.5]), help='mean.')\n",
    "# parser.add_argument('--std', type=float, default=np.array([0.5, 0.5, 0.5]), help='std.')\n",
    "# parser.add_argument(\"--max_epsilon\", type=float, default=16.0, help=\"Maximum size of adversarial perturbation.\")\n",
    "# parser.add_argument(\"--num_iter_set\", type=int, default=10, help=\"Number of iterations.\")\n",
    "# parser.add_argument(\"--image_width\", type=int, default=299, help=\"Width of each input images.\")\n",
    "# parser.add_argument(\"--image_height\", type=int, default=299, help=\"Height of each input images.\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=20, help=\"How many images process at one time.\")\n",
    "# parser.add_argument(\"--momentum\", type=float, default=1.0, help=\"Momentum\")\n",
    "# parser.add_argument(\"--N\", type=int, default=20, help=\"The number of sampled examples\")\n",
    "# parser.add_argument(\"--delta\", type=float, default=0.5, help=\"The balanced coefficient\")\n",
    "# parser.add_argument(\"--zeta\", type=float, default=3.0, help=\"The upper bound of neighborhood\")\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "transforms = T.Compose(\n",
    "    [T.Resize(299), T.ToTensor()]\n",
    ")\n",
    "\n",
    "def clip_by_tensor(t, t_min, t_max):\n",
    "    \"\"\"\n",
    "    clip_by_tensor\n",
    "    :param t: tensor\n",
    "    :param t_min: min\n",
    "    :param t_max: max\n",
    "    :return: cliped tensor\n",
    "    \"\"\"\n",
    "    result = (t >= t_min).float() * t + (t < t_min).float() * t_min\n",
    "    result = (result <= t_max).float() * result + (result > t_max).float() * t_max\n",
    "    return result\n",
    "\n",
    "def save_image(images,names,output_dir):\n",
    "    \"\"\"save the adversarial images\"\"\"\n",
    "    if os.path.exists(output_dir)==False:\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i,name in enumerate(names):\n",
    "        img = Image.fromarray(images[i].astype('uint8'))\n",
    "        img.save(output_dir + name)\n",
    "\n",
    "def PGN(images, gt, model, min, max):\n",
    "    \"\"\"\n",
    "    The attack algorithm of our proposed CMI-FGSM\n",
    "    :param images: the input images\n",
    "    :param gt: ground-truth\n",
    "    :param model: substitute model\n",
    "    :param mix: the mix the clip operation \n",
    "    :param max: the max the clip operation\n",
    "    :return: the adversarial images\n",
    "    \"\"\"\n",
    "    eps = e / 255.0\n",
    "    num_iter = 10\n",
    "    alpha = eps / num_iter\n",
    "    momentum = 1\n",
    "    x = images.clone().detach().cuda()\n",
    "    zeta = 3\n",
    "    delta = 0.5\n",
    "    N = 20\n",
    "    grad = torch.zeros_like(x).detach().cuda()\n",
    "    for i in range(num_iter):\n",
    "        avg_grad = torch.zeros_like(x).detach().cuda()\n",
    "        for _ in range(N):\n",
    "            x_near = x + torch.rand_like(x).uniform_(-eps*zeta, eps*zeta)\n",
    "            x_near = V(x_near, requires_grad = True)\n",
    "            output_v3 = model(x_near)\n",
    "            loss = F.cross_entropy(output_v3, gt)\n",
    "            g1 = torch.autograd.grad(loss, x_near,\n",
    "                                        retain_graph=False, create_graph=False)[0]\n",
    "            x_star = x_near.detach() + alpha * (-g1)/torch.abs(g1).mean([1, 2, 3], keepdim=True)\n",
    "\n",
    "            nes_x = x_star.detach()\n",
    "            nes_x = V(nes_x, requires_grad = True)\n",
    "            output_v3 = model(nes_x)\n",
    "            loss = F.cross_entropy(output_v3, gt)\n",
    "            g2 = torch.autograd.grad(loss, nes_x,\n",
    "                                        retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "            avg_grad += (1-delta)*g1 + delta*g2\n",
    "        noise = (avg_grad) / torch.abs(avg_grad).mean([1, 2, 3], keepdim=True)\n",
    "        noise = momentum * grad + noise\n",
    "        grad = noise\n",
    "        \n",
    "        x = x + alpha * torch.sign(noise)\n",
    "        x = clip_by_tensor(x, min, max)\n",
    "    return x.detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df629dca-a652-47b7-80bf-eb49dc3c10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________\n",
      "Victim: vit_b_16\n",
      "g: efficientnet_b0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "835\n",
      "g: swin_s\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "batch_size = 100\n",
    "d = {}\n",
    "for i , (name_v, model_v, weights_v) in enumerate(models): \n",
    "        victim = model_v(weights=weights_v.DEFAULT)\n",
    "        print(\"______________________\")\n",
    "        print(\"Victim: \"+ name_v)\n",
    "    \n",
    "        S = models.copy()\n",
    "        S.pop(i)\n",
    "        victim = model_with_normalization(victim, normalization)\n",
    "        victim.eval()\n",
    "        victim = victim.to(device)\n",
    "        \n",
    "        with open('../data_'+name_v+'1000images.pkl', 'rb') as f:\n",
    "            new_data_loader = pickle.load(f)\n",
    "    \n",
    "        for j , (name_g,model_g, weights_g) in enumerate(S):\n",
    "                       row = [0,0,0]\n",
    "                       print(\"g: \"+ name_g) \n",
    "                       g = model_g(weights=weights_g.DEFAULT) \n",
    "                       Se = S.copy()\n",
    "                       Se.pop(j)\n",
    "                       g = model_with_normalization(g, normalization)\n",
    "                       g.eval()\n",
    "                       g = g.to(device)\n",
    "                       ASR = 0\n",
    "                       l2_score = 0\n",
    "            \n",
    "                       for batch , (images, labels) in enumerate(new_data_loader):\n",
    "                           print(batch)\n",
    "                           labels =labels.to(device)\n",
    "                           #PGN\n",
    "                           images_min = clip_by_tensor(images - e / 255.0, 0.0, 1.0)\n",
    "                           images_max = clip_by_tensor(images + e / 255.0, 0.0, 1.0)\n",
    "                           x_p = PGN(images, labels, g, images_min, images_max)\n",
    "                           adv_label_p = victim(x_p).argmax(dim=1)\n",
    "                           # adv_label_p = adv_label_p.to('cpu')\n",
    "                           ASR += (labels != adv_label_p).int().sum().item()\n",
    "                           l2_score += torch.norm(images - x_p)\n",
    "                           #print(l2_score)\n",
    "            \n",
    "                       d[f'{name_v}_{name_g}_ASR'] = ASR/1000\n",
    "                       d[f'{name_v}_{name_g}_L2'] = l2_score/1000\n",
    "                       print(ASR)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48005df-227b-4046-9995-bf8f4c00a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ranking_simba_baselines/PGN.pkl', 'wb') as f:\n",
    "     pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c957c-91cf-431f-8de9-8d6e72303e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGN\n",
    "batch_size = 100\n",
    "for i , (name_v, model_v, weights_v) in enumerate(models): \n",
    "    if i==3:\n",
    "        victim = model_v(weights=weights_v.DEFAULT)\n",
    "        print(\"______________________\")\n",
    "        print(\"Victim: \"+ name_v)\n",
    "    \n",
    "        S = models.copy()\n",
    "        S.pop(i)\n",
    "        victim = model_with_normalization(victim, normalization)\n",
    "        victim.eval()\n",
    "        victim = victim.to(device)\n",
    "        \n",
    "        with open('../data_'+name_v+'1000images.pkl', 'rb') as f:\n",
    "            new_data_loader = pickle.load(f)\n",
    "    \n",
    "        # new_data_loader = DataLoader(\n",
    "        #         new_data_loader.dataset[x:x+100], batch_size=batch_size, shuffle=True\n",
    "        #     )\n",
    "        \n",
    "    \n",
    "        for j , (name_g,model_g, weights_g) in enumerate(S):\n",
    "                       row = [0,0,0]\n",
    "                       print(\"g: \"+ name_g) \n",
    "                       g = model_g(weights=weights_g.DEFAULT) \n",
    "                       Se = S.copy()\n",
    "                       Se.pop(j)\n",
    "                       g = model_with_normalization(g, normalization)\n",
    "    \n",
    "                       g.eval()\n",
    "                       g = g.to(device)\n",
    "                       ASR_total = 0\n",
    "                       for batch , (images, labels) in enumerate(new_data_loader):\n",
    "                           labels =labels.to(device)\n",
    "                           perturbation_confidences = []\n",
    "                           perturbation_success = []\n",
    "    \n",
    "                           augmentations_confidences = []\n",
    "                           augmentations_success = []\n",
    "    \n",
    "                           augmentation_confidences = []\n",
    "                           augmentation_success = [] \n",
    "    \n",
    "                           confidences = []\n",
    "    \n",
    "                           print(batch)\n",
    "    \n",
    "                           \n",
    "                           #PGN\n",
    "                           images_min = clip_by_tensor(images - e / 255.0, 0.0, 1.0)\n",
    "                           images_max = clip_by_tensor(images + e / 255.0, 0.0, 1.0)\n",
    "                           x_p = PGN(images, labels, g, images_min, images_max)\n",
    "                           adv_label_p = victim(x_p).argmax(dim=1)\n",
    "                           # adv_label_p = adv_label_p.to('cpu')\n",
    "                           ASR = (labels != adv_label_p).int().sum().item()\n",
    "                           print(ASR)\n",
    "                           row[0] += ASR\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                           \n",
    "                           for _ in tqdm(range(200)):\n",
    "                               \n",
    "                                # PGD_attack = PGD(g, eps=e/255, alpha=0.3/255, steps =20, random_start=True)\n",
    "    \n",
    "                               #PGN+ s2\n",
    "                                \n",
    "                                adv_label_p = victim(x_p).argmax(dim=1)\n",
    "                                adv_label_p = adv_label_p.to('cpu')\n",
    "                                # mixing augmentations\n",
    "                                x_a = augmentations(images)\n",
    "                                images_min = clip_by_tensor(x_a - e / 255.0, 0.0, 1.0)\n",
    "                                images_max = clip_by_tensor(x_a + e / 255.0, 0.0, 1.0)\n",
    "                                x_a = PGN(x_a, labels, g, images_min, images_max)\n",
    "                                confidance = victim(x_a).softmax(dim=1)\n",
    "                                conf = confidance[torch.arange(confidance.size(0)),labels].detach().cpu().numpy()\n",
    "                                adv_label_a = confidance.argmax(dim=1)\n",
    "                                # adv_label_a = adv_label_a.to('cpu')\n",
    "                                del confidance\n",
    "    \n",
    "                                torch.cuda.empty_cache()\n",
    "    \n",
    "                                # average_confidences_p =  np.zeros(batch_size, dtype=np.float32)\n",
    "                                average_confidences_a =  np.zeros(batch_size, dtype=np.float32)\n",
    "                                # average_confidences_o =  np.zeros(batch_size, dtype=np.float32)\n",
    "                                for n , model_s, weights_s in Se:\n",
    "    \n",
    "                                    model = model_s(weights=weights_s.DEFAULT)\n",
    "                                    model = model.to(device)\n",
    "                                    model = model_with_normalization(model, normalization)\n",
    "                                    model.eval()\n",
    "    \n",
    "    \n",
    "                                    outputs_a = model(x_a).softmax(dim=1)\n",
    "                                    confidences_a = outputs_a[torch.arange(outputs_a.size(0)),labels].detach().cpu().numpy()                            \n",
    "                                    average_confidences_a += confidences_a\n",
    "                                    del outputs_a,confidences_a\n",
    "    \n",
    "                                    del model\n",
    "    \n",
    "    \n",
    "                                    torch.cuda.empty_cache()\n",
    "                                del x_a#x_p,x_a,x_o\n",
    "    \n",
    "    \n",
    "    \n",
    "                                # average_confidences_p /= len(Se)\n",
    "                                average_confidences_a /= len(Se)\n",
    "                                # average_confidences_o /= len(Se)\n",
    "    \n",
    "                                # equality_tensor_p = (labels != adv_label_p).int()\n",
    "                                equality_tensor_a = (labels != adv_label_a).int() \n",
    "                                # equality_tensor_o = (labels != adv_label_o).int()\n",
    "    \n",
    "    \n",
    "    \n",
    "                                confidences.append(conf)\n",
    "    \n",
    "                                # perturbation_confidences.append(average_confidences_p)\n",
    "                                # perturbation_success.append(equality_tensor_p)\n",
    "                                augmentations_confidences.append(average_confidences_a)\n",
    "                                augmentations_success.append(equality_tensor_a)\n",
    "                                # augmentation_confidences.append(average_confidences_o)\n",
    "                                # augmentation_success.append(equality_tensor_o)\n",
    "    \n",
    "                                torch.cuda.empty_cache()\n",
    "                         \n",
    "                           merged_results_a = [\n",
    "                                                tuple(\n",
    "                                                    zip(\n",
    "                                                        np.array(augmentations_confidences)[:, i],\n",
    "                                                        np.array(confidences)[:, i],\n",
    "                                                        torch.stack(augmentations_success).cpu().numpy()[:, i]\n",
    "                                                    )\n",
    "                                                ) \n",
    "                                                for i in range(batch_size)\n",
    "                                            ]\n",
    "    \n",
    "                           sorted_results_a = [sorted(image_results, key=lambda x: x[0], reverse=False) for image_results in merged_results_a]\n",
    "    \n",
    "                           \n",
    "    \n",
    "                           best_result = 0\n",
    "                           random_result = 0 \n",
    "                           best_v = 0\n",
    "                           for sublist in sorted_results_a:\n",
    "    \n",
    "                                best_result += sublist[0][2]\n",
    "                                # best_v += min(sublist, key=lambda x: x[1])[2]\n",
    "                                # Increment the best result based on the first item's binary value\n",
    "                                #best_result += sublist[0][1]\n",
    "                                # Calculate the sum of random items in each sublist\n",
    "                                random_result += random.choice(sublist)[2]\n",
    "    \n",
    "    \n",
    "                           row[1]+=random_result\n",
    "                           row[2]+=best_result\n",
    "                           # row[6]+=best_v\n",
    "                           print(row) \n",
    "                           # upperB[(name_v,name_g)] += best_v  \n",
    "                           # print(results_a)     \n",
    "                           # print(upperB)  \n",
    "                           del images, labels\n",
    "                           torch.cuda.empty_cache()\n",
    "                            \n",
    "    \n",
    "               \n",
    "                       with open('PGN_all/'+name_v+'_'+name_g+'_'+str(e)+'.pkl', 'wb') as f:\n",
    "                         pickle.dump(row, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52793e19-424e-4e96-bf66-178ca6327d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# p=0\n",
    "# c=0\n",
    "# for i in range(10):\n",
    "#     with open('SSA/efficientnet_b0_vit_b_16_25.5'+str(i*100)+'.pkl', 'rb') as f:\n",
    "#         l = pickle.load(f)\n",
    "#         c+=l[0]\n",
    "#         p+=l[2]\n",
    "# print(p)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79fe51-5573-46b2-b7b6-31ff083d2f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST_GPU",
   "language": "python",
   "name": "test_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
