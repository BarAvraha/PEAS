{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d5aa27-12e1-43e7-9eca-84a811b826ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "# from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
    "from torchattacks import PGD\n",
    "from art.utils import random_sphere\n",
    "from art.config import ART_NUMPY_DTYPE\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, vgg19 , VGG19_Weights, resnet18, ResNet18_Weights, densenet121,  densenet121, DenseNet121_Weights, vit_b_16, ViT_B_16_Weights, efficientnet_b2, EfficientNet_B2_Weights,  swin_s, Swin_S_Weights, convnext_base, ConvNeXt_Base_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c49aec-a35a-48a8-a12f-6c808129853e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4049f8-efa8-428e-9ad1-edd186f741e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [(\"vit_b_16\",vit_b_16, ViT_B_16_Weights),\n",
    "          (\"efficientnet_b0\" , efficientnet_b0, EfficientNet_B0_Weights),\n",
    "          (\"swin_s\", swin_s, Swin_S_Weights),\n",
    "          (\"densenet121\" , densenet121, DenseNet121_Weights),\n",
    "          (\"resnet18\" ,resnet18, ResNet18_Weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbacab9c-19b5-4c8b-919a-e812f394826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_with_normalization(nn.Module):\n",
    "    def __init__(self, model, normalization):\n",
    "        super(model_with_normalization, self).__init__()\n",
    "        self.model = model \n",
    "        self.normalization = normalization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)\n",
    "        if x.shape == (3,224,224):\n",
    "          x = x.unsqueeze(0)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f85c398-5f55-4665-9605-1c8188208474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Initialize your custom dataset\n",
    "# new_dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d072519-9255-409b-86bd-de643cf905d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac1080b-b14c-4713-a3ef-86bc9e2d558b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from torch.cuda.amp import autocast\n",
    "trans = [\n",
    "    transforms.RandomAffine(degrees=(-2, 2), translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(size=(224, 224), padding=10),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    transforms.RandomAutocontrast()]\n",
    "\n",
    "\n",
    "augmentations = transforms.Compose(trans)\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acfa34f-4a91-4cca-9a4d-4d12e0512b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 12.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c957c-91cf-431f-8de9-8d6e72303e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________\n",
      "Victim: vit_b_16\n",
      "12.75\n",
      "g: efficientnet_b0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [24:29<00:00,  7.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 10, 21, 36, 16, 21, 16, 34]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [24:29<00:00,  7.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 25, 45, 76, 23, 41, 25, 67]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [24:04<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 38, 64, 108, 37, 65, 38, 97]\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [08:05<14:36,  6.64s/it]"
     ]
    }
   ],
   "source": [
    "#ALGORITHM COMPONENTS\n",
    "for i , (name_v, model_v, weights_v) in enumerate(models): \n",
    "    if i !=0 :\n",
    "        continue\n",
    "    victim = model_v(weights=weights_v.DEFAULT)\n",
    "    print(\"______________________\")\n",
    "    print(\"Victim: \"+ name_v)\n",
    "\n",
    "    S = models.copy()\n",
    "    S.pop(i)\n",
    "    victim = model_with_normalization(victim, normalization)\n",
    "    victim.eval()\n",
    "    victim = victim.to(device)\n",
    "    \n",
    "    with open('data_'+name_v+'1000images.pkl', 'rb') as f:\n",
    "        new_data_loader = pickle.load(f)\n",
    "\n",
    "    #new_data_loader = DataLoader(new_data_loader.dataset, batch_size=batch_size, shuffle=True)\n",
    "    results = []\n",
    "    print(e)\n",
    "    for j , (name_g,model_g, weights_g) in enumerate(S):\n",
    "               row = [0,0,0,0,0,0,0,0]\n",
    "               print(\"g: \"+ name_g) \n",
    "               g = model_g(weights=weights_g.DEFAULT) \n",
    "               Se = S.copy()\n",
    "               Se.pop(j)\n",
    "               g = model_with_normalization(g, normalization)\n",
    "\n",
    "               g.eval()\n",
    "               g = g.to(device)\n",
    "               for batch , (images, labels) in enumerate(new_data_loader):\n",
    "\n",
    "                   augmentations_confidences = []\n",
    "                   augmentations_success = []\n",
    "\n",
    "                   augmentation_confidences = []\n",
    "                   augmentation_success = [] \n",
    "\n",
    "                   augmentations_confidences_attack = []\n",
    "                   augmentations_success_attack = []\n",
    "\n",
    "                   augmentation_confidences_attack = []\n",
    "                   augmentation_success_attack = [] \n",
    "\n",
    "                   print(batch)\n",
    "\n",
    "                   if batch==10:\n",
    "                        break\n",
    "                   for _ in tqdm(range(200)):\n",
    "\n",
    "                        victim = victim.to(device)\n",
    "                        PGD_attack = PGD(g, eps=e/255, alpha=random.uniform(0.1/255, 0.3/255), steps =random.randint(10, 20), random_start=True)\n",
    "\n",
    "                        #mixing augmentations\n",
    "                        x_a = augmentations(images)\n",
    "                        confidance = victim(x_a).softmax(dim=1)\n",
    "                        adv_label_a = confidance.argmax(dim=1)  \n",
    "                        adv_label_a = adv_label_a.to('cpu')\n",
    "\n",
    "\n",
    "                        x_attack_a = PGD_attack(x_a, labels)\n",
    "                        attack_label_a = victim(x_attack_a).softmax(dim=1).argmax(dim=1)\n",
    "                        attack_label_a = attack_label_a.to('cpu')\n",
    "\n",
    "\n",
    "                        #one augmentation\n",
    "                        x_o = random.choice(trans)(images)\n",
    "                        adv_label_o = victim(x_o).argmax(dim=1)\n",
    "                        adv_label_o = adv_label_o.to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "                        x_attack_o = PGD_attack(x_a, labels)\n",
    "                        attack_label_o = victim(x_attack_o).softmax(dim=1).argmax(dim=1)\n",
    "                        attack_label_o = attack_label_o.to('cpu')\n",
    "\n",
    "                        torch.cuda.empty_cache()\n",
    "                        del PGD_attack\n",
    "                        victim = victim.to('cpu')\n",
    "\n",
    "                        # average_confidences_p =  np.zeros(batch_size, dtype=np.float32)\n",
    "                        average_confidences_a =  np.zeros(batch_size, dtype=np.float32)\n",
    "                        average_confidences_o =  np.zeros(batch_size, dtype=np.float32)\n",
    "\n",
    "\n",
    "                        average_confidences_attack_a =  np.zeros(batch_size, dtype=np.float32)\n",
    "                        average_confidences_attack_o =  np.zeros(batch_size, dtype=np.float32)\n",
    "\n",
    "                        for n , model_s, weights_s in Se:\n",
    "\n",
    "                            model = model_s(weights=weights_s.DEFAULT)\n",
    "                            model = model.to(device)\n",
    "                            model = model_with_normalization(model, normalization)\n",
    "                            model.eval()\n",
    "\n",
    "\n",
    "\n",
    "                            outputs_a = model(x_a).softmax(dim=1)\n",
    "                            confidences_a = outputs_a[torch.arange(outputs_a.size(0)),labels].detach().cpu().numpy()                            \n",
    "                            average_confidences_a += confidences_a\n",
    "\n",
    "\n",
    "                            outputs_a = model(x_attack_a).softmax(dim=1)\n",
    "                            confidences_a = outputs_a[torch.arange(outputs_a.size(0)),labels].detach().cpu().numpy()                            \n",
    "                            average_confidences_attack_a += confidences_a\n",
    "\n",
    "                            del outputs_a,confidences_a\n",
    "\n",
    "\n",
    "                            outputs_o = model(x_o).softmax(dim=1)\n",
    "                            confidences_o = outputs_o[torch.arange(outputs_o.size(0)),labels].detach().cpu().numpy()                            \n",
    "                            average_confidences_o += confidences_o\n",
    "\n",
    "                            outputs_o = model(x_attack_o).softmax(dim=1)\n",
    "                            confidences_o = outputs_o[torch.arange(outputs_o.size(0)),labels].detach().cpu().numpy()                            \n",
    "                            average_confidences_attack_o += confidences_o\n",
    "\n",
    "\n",
    "                            del outputs_o,confidences_o,model\n",
    "\n",
    "\n",
    "\n",
    "                            torch.cuda.empty_cache()\n",
    "                        del x_a,x_o,x_attack_o,x_attack_a\n",
    "\n",
    "\n",
    "                        average_confidences_a /= len(Se)\n",
    "                        average_confidences_o /= len(Se)\n",
    "                        equality_tensor_a = (labels != adv_label_a).int() \n",
    "                        equality_tensor_o = (labels != adv_label_o).int()\n",
    "\n",
    "                        average_confidences_attack_a /= len(Se)\n",
    "                        average_confidences_attack_o /= len(Se)\n",
    "\n",
    "                        equality_tensor_attack_a = (labels != attack_label_a).int() \n",
    "                        equality_tensor_attack_o = (labels != attack_label_o).int() #1 if succsess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        augmentations_confidences.append(average_confidences_a)\n",
    "                        augmentations_success.append(equality_tensor_a)\n",
    "                        augmentation_confidences.append(average_confidences_o)\n",
    "                        augmentation_success.append(equality_tensor_o)\n",
    "\n",
    "\n",
    "                        augmentations_confidences_attack.append(average_confidences_attack_a)\n",
    "                        augmentations_success_attack.append(equality_tensor_attack_a)\n",
    "                        augmentation_confidences_attack.append(average_confidences_attack_o)\n",
    "                        augmentation_success_attack.append(equality_tensor_attack_o)\n",
    "\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                   merged_results_a = [tuple(zip(np.array(augmentations_confidences)[:, i], torch.stack(augmentations_success).numpy()[:, i]))\n",
    "                        for i in range(batch_size)]\n",
    "\n",
    "                   merged_results_o = [tuple(zip(np.array(augmentation_confidences)[:, i], torch.stack(augmentation_success).numpy()[:, i]))\n",
    "                        for i in range(batch_size)]\n",
    "\n",
    "\n",
    "\n",
    "                   merged_results_attack_a = [tuple(zip(np.array(augmentations_confidences_attack)[:, i],torch.stack(augmentations_success_attack).numpy()[:, i] ,torch.stack(augmentations_success).numpy()[:, i]))\n",
    "                        for i in range(batch_size)]\n",
    "\n",
    "                   merged_results_attack_o = [tuple(zip(np.array(augmentation_confidences_attack)[:, i], torch.stack(augmentation_success_attack).numpy()[:, i] , torch.stack(augmentation_success).numpy()[:, i]))\n",
    "                        for i in range(batch_size)] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                   sorted_results_o = [sorted(image_results, key=lambda x: x[0], reverse=False) for image_results in merged_results_o]\n",
    "\n",
    "                   sorted_results_a = [sorted(image_results, key=lambda x: x[0], reverse=False) for image_results in merged_results_a]\n",
    "\n",
    "\n",
    "                   sorted_results_attack_o = [sorted(image_results, key=lambda x: x[0], reverse=False) for image_results in merged_results_attack_o]\n",
    "\n",
    "                   sorted_results_attack_a = [sorted(image_results, key=lambda x: x[0], reverse=False) for image_results in merged_results_attack_a]\n",
    "\n",
    "\n",
    "                   #filter\n",
    "                   filtered_data_o = [[tuple for tuple in sublist if tuple[-1] == 0] for sublist in sorted_results_attack_o]\n",
    "                   filtered_data_a = [[tuple for tuple in sublist if tuple[-1] == 0] for sublist in sorted_results_attack_a]\n",
    "\n",
    "\n",
    "\n",
    "                   best_result = 0\n",
    "                   random_result = 0 \n",
    "                   for sublist in sorted_results_o:\n",
    "                        # Increment the best result based on the first item's binary value\n",
    "                        best_result += sublist[0][1]\n",
    "                        # Calculate the sum of random items in each sublist\n",
    "                        random_result += random.choice(sublist)[1]\n",
    "                   row[0]+=random_result\n",
    "                   row[1]+=best_result\n",
    "\n",
    "                   best_result = 0\n",
    "                   random_result = 0 \n",
    "                   for sublist in filtered_data_o:\n",
    "                        # Increment the best result based on the first item's binary value\n",
    "                        if len(sublist)>0:\n",
    "                            best_result += sublist[0][1]\n",
    "                            # Calculate the sum of random items in each sublist\n",
    "                            random_result += random.choice(sublist)[1]\n",
    "                   row[2]+=random_result\n",
    "                   row[3]+=best_result\n",
    "\n",
    "\n",
    "\n",
    "                   best_result = 0\n",
    "                   random_result = 0 \n",
    "                   for sublist in sorted_results_a:\n",
    "                        best_result += sublist[0][1]\n",
    "                        random_result += random.choice(sublist)[1]\n",
    "\n",
    "\n",
    "                   row[4]+=random_result\n",
    "                   row[5]+=best_result\n",
    "\n",
    "\n",
    "                   best_result = 0\n",
    "                   random_result = 0 \n",
    "                   for sublist in filtered_data_a:\n",
    "                     if len(sublist)>0:\n",
    "                        best_result += sublist[0][1]\n",
    "                        random_result += random.choice(sublist)[1]\n",
    "\n",
    "\n",
    "                   row[6]+=random_result\n",
    "                   row[7]+=best_result\n",
    "\n",
    "                   print(row) \n",
    "                   del images, labels\n",
    "                   torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "               results.append(row)\n",
    "               print(results)\n",
    "               del g\n",
    "               with open('imagenet_Algorithm_components_results/'+name_v+'_'+name_g+'_'+str(e)+'.pkl', 'wb') as f:\n",
    "                 pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34afee-c4ed-4f6c-ac68-ea8267dc62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , name_v in enumerate(models):\n",
    "       \n",
    "        S = models.copy()\n",
    "        S.pop(i)        \n",
    "        for j , name_g in enumerate(S):\n",
    "            if j==3:\n",
    "                 with open('imagenet_Algorithm_components_results/'+name_v+'_'+name_g+'_'+str(e)+'.pkl', 'rb') as f:\n",
    "                                 results = pickle.load(f)\n",
    "                                 table_e+=results\n",
    "\n",
    "    table_e = [[item / 1000 for item in sublist] for sublist in table_e]\n",
    "\n",
    "    with open('imagenet_Algorithm_components_results/tables/table_'+str(e)+'.pkl', 'wb') as f:\n",
    "                     pickle.dump(table_e, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9223d-afed-4c12-a228-ee1370ef4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['s1_random', 's1_rank', 's1_filter_random', 's1_filter_rank','s2_random', 's2_rank', 's2_filter_random', 's2_filter_rank']\n",
    "\n",
    "df = pd.DataFrame(table_e, columns=columns)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST_GPU",
   "language": "python",
   "name": "test_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
