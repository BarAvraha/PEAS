{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d5aa27-12e1-43e7-9eca-84a811b826ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/baravrah/.conda/envs/gpu_test/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "# from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
    "from torchattacks import PGD\n",
    "from art.utils import random_sphere\n",
    "from art.config import ART_NUMPY_DTYPE\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, vgg19 , VGG19_Weights, resnet18, ResNet18_Weights, densenet121,  densenet121, DenseNet121_Weights, vit_b_16, ViT_B_16_Weights, efficientnet_b2, EfficientNet_B2_Weights,  swin_s, Swin_S_Weights, convnext_base, ConvNeXt_Base_Weights, squeezenet1_0, SqueezeNet1_0_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff6c684-39c6-487b-9f49-6cac58bf103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'570194'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SLURM_JOBID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c49aec-a35a-48a8-a12f-6c808129853e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4049f8-efa8-428e-9ad1-edd186f741e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [(\"vit_b_16\",vit_b_16, ViT_B_16_Weights),\n",
    "          (\"efficientnet_b0\" , efficientnet_b0, EfficientNet_B0_Weights),\n",
    "          (\"swin_s\", swin_s, Swin_S_Weights),\n",
    "          (\"densenet121\" , densenet121, DenseNet121_Weights),\n",
    "          (\"resnet18\" ,resnet18, ResNet18_Weights)]\n",
    "            # (\"convnext_base\", convnext_base,ConvNeXt_Base_Weights)]\n",
    "         # (\"squeezenet1_0\",squeezenet1_0, SqueezeNet1_0_Weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbacab9c-19b5-4c8b-919a-e812f394826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_with_normalization(nn.Module):\n",
    "    def __init__(self, model, normalization):\n",
    "        super(model_with_normalization, self).__init__()\n",
    "        self.model = model \n",
    "        self.normalization = normalization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)\n",
    "        if x.shape == (3,224,224):\n",
    "          x = x.unsqueeze(0)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f85c398-5f55-4665-9605-1c8188208474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Initialize your custom dataset\n",
    "# new_dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d072519-9255-409b-86bd-de643cf905d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    return Image.open(image_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac1080b-b14c-4713-a3ef-86bc9e2d558b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from torch.cuda.amp import autocast\n",
    "trans = [\n",
    "    transforms.RandomAffine(degrees=(-2, 2), translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(size=(224, 224), padding=10),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=1),\n",
    "    transforms.RandomAutocontrast(1)]\n",
    "\n",
    "\n",
    "augmentations = transforms.Compose(trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9acfa34f-4a91-4cca-9a4d-4d12e0512b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=12.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38996182-e919-4b88-a28b-1835f161c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def dct1(x):\n",
    "    \"\"\"\n",
    "    Discrete Cosine Transform, Type I\n",
    "\n",
    "    :param x: the input signal\n",
    "    :return: the DCT-I of the signal over the last dimension\n",
    "    \"\"\"\n",
    "    x_shape = x.shape\n",
    "    x = x.view(-1, x_shape[-1])\n",
    "\n",
    "    return torch.fft.fft(torch.cat([x, x.flip([1])[:, 1:-1]], dim=1), 1).real.view(*x_shape)\n",
    "\n",
    "\n",
    "def idct1(X):\n",
    "    \"\"\"\n",
    "    The inverse of DCT-I, which is just a scaled DCT-I\n",
    "\n",
    "    Our definition if idct1 is such that idct1(dct1(x)) == x\n",
    "\n",
    "    :param X: the input signal\n",
    "    :return: the inverse DCT-I of the signal over the last dimension\n",
    "    \"\"\"\n",
    "    n = X.shape[-1]\n",
    "    return dct1(X) / (2 * (n - 1))\n",
    "\n",
    "\n",
    "def dct(x, norm=None):\n",
    "    \"\"\"\n",
    "    Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param x: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the DCT-II of the signal over the last dimension\n",
    "    \"\"\"\n",
    "    x_shape = x.shape\n",
    "    N = x_shape[-1]\n",
    "    x = x.contiguous().view(-1, N)\n",
    "\n",
    "    v = torch.cat([x[:, ::2], x[:, 1::2].flip([1])], dim=1)\n",
    "\n",
    "    Vc = torch.fft.fft(v)\n",
    "\n",
    "    k = - torch.arange(N, dtype=x.dtype, device=x.device)[None, :] * np.pi / (2 * N)\n",
    "    W_r = torch.cos(k)\n",
    "    W_i = torch.sin(k)\n",
    "\n",
    "    # V = Vc[:, :, 0] * W_r - Vc[:, :, 1] * W_i\n",
    "    V = Vc.real * W_r - Vc.imag * W_i\n",
    "    if norm == 'ortho':\n",
    "        V[:, 0] /= np.sqrt(N) * 2\n",
    "        V[:, 1:] /= np.sqrt(N / 2) * 2\n",
    "\n",
    "    V = 2 * V.view(*x_shape)\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def idct(X, norm=None):\n",
    "    \"\"\"\n",
    "    The inverse to DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
    "\n",
    "    Our definition of idct is that idct(dct(x)) == x\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param X: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the inverse DCT-II of the signal over the last dimension\n",
    "    \"\"\"\n",
    "\n",
    "    x_shape = X.shape\n",
    "    N = x_shape[-1]\n",
    "\n",
    "    X_v = X.contiguous().view(-1, x_shape[-1]) / 2\n",
    "\n",
    "    if norm == 'ortho':\n",
    "        X_v[:, 0] *= np.sqrt(N) * 2\n",
    "        X_v[:, 1:] *= np.sqrt(N / 2) * 2\n",
    "\n",
    "    k = torch.arange(x_shape[-1], dtype=X.dtype, device=X.device)[None, :] * np.pi / (2 * N)\n",
    "    W_r = torch.cos(k)\n",
    "    W_i = torch.sin(k)\n",
    "\n",
    "    V_t_r = X_v\n",
    "    V_t_i = torch.cat([X_v[:, :1] * 0, -X_v.flip([1])[:, :-1]], dim=1)\n",
    "\n",
    "    V_r = V_t_r * W_r - V_t_i * W_i\n",
    "    V_i = V_t_r * W_i + V_t_i * W_r\n",
    "\n",
    "    V = torch.cat([V_r.unsqueeze(2), V_i.unsqueeze(2)], dim=2)\n",
    "    tmp = torch.complex(real=V[:, :, 0], imag=V[:, :, 1])\n",
    "    v = torch.fft.ifft(tmp)\n",
    "\n",
    "    x = v.new_zeros(v.shape)\n",
    "    x[:, ::2] += v[:, :N - (N // 2)]\n",
    "    x[:, 1::2] += v.flip([1])[:, :N // 2]\n",
    "\n",
    "    return x.view(*x_shape).real\n",
    "\n",
    "\n",
    "def dct_2d(x, norm=None):\n",
    "    \"\"\"\n",
    "    2-dimentional Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param x: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the DCT-II of the signal over the last 2 dimensions\n",
    "    \"\"\"\n",
    "    X1 = dct(x, norm=norm)\n",
    "    X2 = dct(X1.transpose(-1, -2), norm=norm)\n",
    "    return X2.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def idct_2d(X, norm=None):\n",
    "    \"\"\"\n",
    "    The inverse to 2D DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
    "\n",
    "    Our definition of idct is that idct_2d(dct_2d(x)) == x\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param X: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the DCT-II of the signal over the last 2 dimensions\n",
    "    \"\"\"\n",
    "    x1 = idct(X, norm=norm)\n",
    "    x2 = idct(x1.transpose(-1, -2), norm=norm)\n",
    "    return x2.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def dct_3d(x, norm=None):\n",
    "    \"\"\"\n",
    "    3-dimentional Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param x: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the DCT-II of the signal over the last 3 dimensions\n",
    "    \"\"\"\n",
    "    X1 = dct(x, norm=norm)\n",
    "    X2 = dct(X1.transpose(-1, -2), norm=norm)\n",
    "    X3 = dct(X2.transpose(-1, -3), norm=norm)\n",
    "    return X3.transpose(-1, -3).transpose(-1, -2)\n",
    "\n",
    "def idct_3d(X, norm=None):\n",
    "    \"\"\"\n",
    "    The inverse to 3D DCT-II, which is a scaled Discrete Cosine Transform, Type III\n",
    "\n",
    "    Our definition of idct is that idct_3d(dct_3d(x)) == x\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param X: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the DCT-II of the signal over the last 3 dimensions\n",
    "    \"\"\"\n",
    "    x1 = idct(X, norm=norm)\n",
    "    x2 = idct(x1.transpose(-1, -2), norm=norm)\n",
    "    x3 = idct(x2.transpose(-1, -3), norm=norm)\n",
    "    return x3.transpose(-1, -3).transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e75e79-cf2b-4b45-a79b-6a4ffe70fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of sample attack.\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "# from attack_methods import DI,gkern\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# from Normalize import Normalize\n",
    "# from loader import ImageNet\n",
    "# from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "# import pretrainedmodels\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--input_csv', type=str, default='./dataset/images.csv', help='Input directory with images.')\n",
    "# parser.add_argument('--input_dir', type=str, default='./dataset/images', help='Input directory with images.')\n",
    "# parser.add_argument('--output_dir', type=str, default='./outputs/', help='Output directory with adversarial images.')\n",
    "# parser.add_argument('--mean', type=float, default=np.array([0.5, 0.5, 0.5]), help='mean.')\n",
    "# parser.add_argument('--std', type=float, default=np.array([0.5, 0.5, 0.5]), help='std.')\n",
    "# parser.add_argument(\"--max_epsilon\", type=float, default=16.0, help=\"Maximum size of adversarial perturbation.\")\n",
    "# parser.add_argument(\"--num_iter_set\", type=int, default=10, help=\"Number of iterations.\")\n",
    "# parser.add_argument(\"--image_width\", type=int, default=299, help=\"Width of each input images.\")\n",
    "# parser.add_argument(\"--image_height\", type=int, default=299, help=\"Height of each input images.\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=10, help=\"How many images process at one time.\")\n",
    "# parser.add_argument(\"--momentum\", type=float, default=1.0, help=\"Momentum\")\n",
    "# parser.add_argument(\"--N\", type=int, default=20, help=\"The number of Spectrum Transformations\")\n",
    "# parser.add_argument(\"--rho\", type=float, default=0.5, help=\"Tuning factor\")\n",
    "# parser.add_argument(\"--sigma\", type=float, default=16.0, help=\"Std of random noise\")\n",
    "\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "transforms = T.Compose(\n",
    "    [T.Resize(299), T.ToTensor()]\n",
    ")\n",
    "\n",
    "def clip_by_tensor(t, t_min, t_max):\n",
    "    \"\"\"\n",
    "    clip_by_tensor\n",
    "    :param t: tensor\n",
    "    :param t_min: min\n",
    "    :param t_max: max\n",
    "    :return: cliped tensor\n",
    "    \"\"\"\n",
    "    result = (t >= t_min).float() * t + (t < t_min).float() * t_min\n",
    "    result = (result <= t_max).float() * result + (result > t_max).float() * t_max\n",
    "    return result\n",
    "\n",
    "def save_image(images,names,output_dir):\n",
    "    \"\"\"save the adversarial images\"\"\"\n",
    "    if os.path.exists(output_dir)==False:\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i,name in enumerate(names):\n",
    "        img = Image.fromarray(images[i].astype('uint8'))\n",
    "        img.save(output_dir + name)\n",
    "\n",
    "# T_kernel = gkern(7, 3)\n",
    "\n",
    "def Spectrum_Simulation_Attack(images, gt, model, min, max):\n",
    "    \"\"\"\n",
    "    The attack algorithm of our proposed Spectrum Simulate Attack\n",
    "    :param images: the input images\n",
    "    :param gt: ground-truth\n",
    "    :param model: substitute model\n",
    "    :param mix: the mix the clip operation \n",
    "    :param max: the max the clip operation\n",
    "    :return: the adversarial images\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    image_width = 224\n",
    "    momentum = 1\n",
    "    num_iter = 10\n",
    "    eps = e / 255.0\n",
    "    alpha = eps / num_iter\n",
    "    x = images.clone()\n",
    "    grad = 0\n",
    "    rho = 0.5\n",
    "    N = 20\n",
    "    sigma = 16\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        noise = 0\n",
    "        for n in range(N):\n",
    "            gauss = torch.randn(x.size()[0], 3, image_width, image_width) * (sigma / 255)\n",
    "            gauss = gauss.cuda()\n",
    "            x_dct = dct_2d(x + gauss).cuda()\n",
    "            mask = (torch.rand_like(x) * 2 * rho + 1 - rho).cuda()\n",
    "            x_idct = idct_2d(x_dct * mask)\n",
    "            x_idct = V(x_idct, requires_grad = True)\n",
    "\n",
    "            # DI-FGSM https://arxiv.org/abs/1803.06978\n",
    "            # output_v3 = model(DI(x_idct))\n",
    "\n",
    "            output_v3 = model(x_idct)\n",
    "            loss = F.cross_entropy(output_v3, gt)\n",
    "            loss.backward()\n",
    "            noise += x_idct.grad.data\n",
    "        noise = noise / N\n",
    "\n",
    "        x = x + alpha * torch.sign(noise)\n",
    "        x = clip_by_tensor(x, min, max)\n",
    "    return x.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b56d0-547b-4000-b23e-13cb404d15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________\n",
      "Victim: vit_b_16\n",
      "g: efficientnet_b0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "419\n",
      "g: swin_s\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "batch_size = 100\n",
    "d = {}\n",
    "for i , (name_v, model_v, weights_v) in enumerate(models): \n",
    "        victim = model_v(weights=weights_v.DEFAULT)\n",
    "        print(\"______________________\")\n",
    "        print(\"Victim: \"+ name_v)\n",
    "    \n",
    "        S = models.copy()\n",
    "        S.pop(i)\n",
    "        victim = model_with_normalization(victim, normalization)\n",
    "        victim.eval()\n",
    "        victim = victim.to(device)\n",
    "        \n",
    "        with open('../data_'+name_v+'1000images.pkl', 'rb') as f:\n",
    "            new_data_loader = pickle.load(f)\n",
    "    \n",
    "        for j , (name_g,model_g, weights_g) in enumerate(S):\n",
    "                       row = [0,0,0]\n",
    "                       print(\"g: \"+ name_g) \n",
    "                       g = model_g(weights=weights_g.DEFAULT) \n",
    "                       Se = S.copy()\n",
    "                       Se.pop(j)\n",
    "                       g = model_with_normalization(g, normalization)\n",
    "                       g.eval()\n",
    "                       g = g.to(device)\n",
    "                       ASR = 0\n",
    "                       total_l2_score = 0\n",
    "            \n",
    "                       for batch , (images, labels) in enumerate(new_data_loader):\n",
    "                           print(batch)\n",
    "                           labels =labels.to(device)\n",
    "                           #PGN\n",
    "                           images_min = clip_by_tensor(images - e / 255.0, 0.0, 1.0)\n",
    "                           images_max = clip_by_tensor(images + e / 255.0, 0.0, 1.0)\n",
    "                           x_p = Spectrum_Simulation_Attack(images, labels, g, images_min, images_max)\n",
    "                           adv_label_p = victim(x_p).argmax(dim=1)\n",
    "                           # adv_label_p = adv_label_p.to('cpu')\n",
    "                           ASR += (labels != adv_label_p).int().sum().item()\n",
    "                           #print(ASR)\n",
    "                           l2_score = torch.norm(images - x_p)\n",
    "                           total_l2_score += l2_score.sum().item()\n",
    "                        \n",
    "\n",
    "            \n",
    "                       d[f'{name_v}_{name_g}_ASR'] = ASR/1000\n",
    "                       d[f'{name_v}_{name_g}_L2'] = total_l2_score/1000\n",
    "                       print(ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895addd-2eb7-4777-8dd7-96aa60698d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ranking_simba_baselines/SSA.pkl', 'wb') as f:\n",
    "     pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879d5579-31c9-4fd2-a9b7-71dd70ddffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947d525-ae8b-4663-8396-dbb88a7423df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________\n",
      "Victim: efficientnet_b0\n",
      "g: vit_b_16\n",
      "0\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 134/200 [4:14:32<2:05:10, 113.79s/it]"
     ]
    }
   ],
   "source": [
    "#SSA\n",
    "\n",
    "batch_size = 100\n",
    "for i , (name_v, model_v, weights_v) in enumerate(models): \n",
    "    if i==0:\n",
    "        continue\n",
    "    if i>1:\n",
    "        break\n",
    "    victim = model_v(weights=weights_v.DEFAULT)\n",
    "    print(\"______________________\")\n",
    "    print(\"Victim: \"+ name_v)\n",
    "\n",
    "    S = models.copy()\n",
    "    S.pop(i)\n",
    "    victim = model_with_normalization(victim, normalization)\n",
    "    victim.eval()\n",
    "    victim = victim.to(device)\n",
    "    \n",
    "    with open('../data_'+name_v+'1000images.pkl', 'rb') as f:\n",
    "        new_data_loader = pickle.load(f)\n",
    "\n",
    "    new_data_loader = DataLoader(\n",
    "            new_data_loader.dataset[x:x+100], batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "    \n",
    "    for j , (name_g,model_g, weights_g) in enumerate(S):\n",
    "                   if j>0:\n",
    "                       break\n",
    "                   row = [0,0,0]\n",
    "                   print(\"g: \"+ name_g) \n",
    "                   g = model_g(weights=weights_g.DEFAULT) \n",
    "                   Se = S.copy()\n",
    "                   Se.pop(j)\n",
    "                   g = model_with_normalization(g, normalization)\n",
    "\n",
    "                   g.eval()\n",
    "                   g = g.to(device)\n",
    "                   ASR_total = 0\n",
    "                   for batch , (images, labels) in enumerate(new_data_loader):\n",
    "                       if batch==1:\n",
    "                            break\n",
    "                       labels =labels.to(device)\n",
    "                       augmentations_confidences = []\n",
    "                       augmentations_success = []\n",
    "\n",
    "                       print(batch)\n",
    "\n",
    "                       \n",
    "                       #SSA\n",
    "                       images_min = clip_by_tensor(images - e / 255.0, 0.0, 1.0)\n",
    "                       images_max = clip_by_tensor(images + e / 255.0, 0.0, 1.0)\n",
    "                       # Spectrum_Simulation_Attack(images, gt, model, images_min, images_max)\n",
    "                       x_p = Spectrum_Simulation_Attack(images, labels, g, images_min, images_max)\n",
    "                       adv_label_p = victim(x_p).argmax(dim=1)\n",
    "                       # adv_label_p = adv_label_p.to('cpu')\n",
    "                       ASR = (labels != adv_label_p).int().sum().item()\n",
    "                       print(ASR)\n",
    "                       row[0] += ASR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                       \n",
    "                       for _ in tqdm(range(200)):\n",
    "                            victim = victim.to(device)\n",
    "                            adv_label_p = victim(x_p).argmax(dim=1)\n",
    "                            adv_label_p = adv_label_p.to('cpu')\n",
    "                            # mixing augmentations\n",
    "                            x_a = augmentations(images)\n",
    "                            images_min = clip_by_tensor(x_a - e / 255.0, 0.0, 1.0)\n",
    "                            images_max = clip_by_tensor(x_a + e / 255.0, 0.0, 1.0)\n",
    "                            x_a = Spectrum_Simulation_Attack(images, labels, g, images_min, images_max)\n",
    "                            confidance = victim(x_a).softmax(dim=1)\n",
    "                            # conf = confidance[torch.arange(confidance.size(0)),labels].detach().cpu().numpy()\n",
    "                            adv_label_a = confidance.argmax(dim=1)\n",
    "\n",
    "\n",
    "                            torch.cuda.empty_cache()\n",
    "                            # victim = victim.to('cpu')\n",
    "\n",
    "                            # average_confidences_p =  np.zeros(batch_size, dtype=np.float32)\n",
    "                            average_confidences_a =  np.zeros(batch_size, dtype=np.float32)\n",
    "                            # average_confidences_o =  np.zeros(batch_size, dtype=np.float32)\n",
    "                            for n , model_s, weights_s in Se:\n",
    "\n",
    "                                model = model_s(weights=weights_s.DEFAULT)\n",
    "                                model = model.to(device)\n",
    "                                model = model_with_normalization(model, normalization)\n",
    "                                model.eval()\n",
    "\n",
    "\n",
    "                                outputs_a = model(x_a).softmax(dim=1)\n",
    "                                confidences_a = outputs_a[torch.arange(outputs_a.size(0)),labels].detach().cpu().numpy()                            \n",
    "                                average_confidences_a += confidences_a\n",
    "                                del outputs_a,confidences_a\n",
    "                                del model\n",
    "\n",
    "\n",
    "                                torch.cuda.empty_cache()\n",
    "                            del x_a\n",
    "                           \n",
    "                            average_confidences_a /= len(Se)\n",
    "                            equality_tensor_a = (labels != adv_label_a).int() \n",
    "\n",
    "\n",
    "\n",
    "                            augmentations_confidences.append(average_confidences_a)\n",
    "                            augmentations_success.append(equality_tensor_a)\n",
    "                            torch.cuda.empty_cache()\n",
    "                 \n",
    "                       merged_results_a = [\n",
    "                                            tuple(\n",
    "                                                zip(\n",
    "                                                    np.array(augmentations_confidences)[:, i],\n",
    "                                                    torch.stack(augmentations_success).cpu().numpy()[:, i]\n",
    "                                                )\n",
    "                                            ) \n",
    "                                            for i in range(batch_size)\n",
    "                                        ]\n",
    "\n",
    "\n",
    " \n",
    "                       sorted_results_a = [sorted(image_results, key=lambda x: x[0], reverse=False) for image_results in merged_results_a]\n",
    "\n",
    "                       best_result = 0\n",
    "                       random_result = 0 \n",
    "                       for sublist in sorted_results_a:\n",
    "\n",
    "                            best_result += sublist[0][1]\n",
    "\n",
    "                            random_result += random.choice(sublist)[1]\n",
    "\n",
    "\n",
    "                       row[1]+=random_result\n",
    "                       row[2]+=best_result\n",
    "\n",
    "                       print(row) \n",
    "                       del images, labels\n",
    "                       torch.cuda.empty_cache()\n",
    "                    \n",
    "                   with open('SSA/'+name_v+'_'+name_g+'_'+str(e)+str(x)+'.pkl', 'wb') as f:\n",
    "                     pickle.dump(row, f)\n",
    "                   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eb5d3-beb1-467b-9e5d-f42a6be586f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST_GPU",
   "language": "python",
   "name": "test_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
